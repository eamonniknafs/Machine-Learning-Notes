# Recall: Linear Regression
### Hypothesis:
$h_\theta(x)=\theta_0+\theta_1x$
$\theta_i$'s: Parameters

### Cost Function
$$J(\theta_0,\theta_1)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$$
SSD = sum of squared differences, also known as
SSE = sum of squared errors

# Multidimensional Inputs
![Pasted image 20221018150715.png|600](Pasted%20image%2020221018150715.png%7C600.md)
### Notation
$n$ = number of features
$x^{(i)}$ = input (features) of $i^{th}$ training example.
$x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example.

# Multivariate Linear Regression
### Hypothesis
$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n$
For convenience of notation, define $x_0=1$
$\theta_i$: Parameters

### Cost Function
$$J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$$
### Goal
 $\underset{\theta_0,\theta_1,...,\theta_n}{minimize}\;J(\theta_0,\theta_1,...,\theta_n)$

## How? Two potential solutions
$\underset{\theta}{min}\;J(\theta;x^{(1)},y^{(1)},...,^{(m)},y^{(m)})$

### Gradient descent (or other iterative algorithm)
- Start with a guess for θ
- Change θ to decrease JJ(θ)
- Until reach minimum

### Direct minimization
- Take derivative, set to zero
- Sufficient condition for minima
- Not possible for most “interesting” cost functions

# Indirect Solution for Linear Regression
## Gradient Descent Algorithm
Set $\theta=0$
Repeat {
$\theta_j:=\theta_j-\alpha\frac{\delta}{\delta\theta_j}J(\theta)$ simultaneously for all $j=0,...,n$
} until convergence

## Gradient Descent: Intuition
![Pasted image 20221021191608.png|600](Pasted%20image%2020221021191608.png%7C600.md)
![Pasted image 20221021191633.png|600](Pasted%20image%2020221021191633.png%7C600.md)

## 2-dimensional parameters
![Pasted image 20221021191728.png|400](Pasted%20image%2020221021191728.png%7C400.md)![Pasted image 20221021191740.png|400](Pasted%20image%2020221021191740.png%7C400.md)
## Gradient Descent for Least Squares Cost
![Pasted image 20221021192115.png|600](Pasted%20image%2020221021192115.png%7C600.md)
Gradient descent computational complexity is intuitively $O(mn)$.

# Feature Normalization
- If features have very different scale, GD can get “stuck” since $x_j$ affects size of gradient in the direction of $j^{th}$ dimension
- Normalizing features to be zero-mean ($\mu$) and same-variance ($\sigma$) helps gradient descent converge faster

![Pasted image 20221022150135.png|600](Pasted%20image%2020221022150135.png%7C600.md)

# Direct Solution for Linear Regression
### Want to minimize SSD
$J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$

### Find minima of function
$\theta \in \mathbb{R}^{n+1}$
$\frac{\delta}{\delta\theta_j}J(\theta)=...=0$ (for every $j$)
Solve for $\theta_0,\theta_1,...,\theta_n$
![Pasted image 20221022151635.png|200](Pasted%20image%2020221022151635.png%7C200.md)

# Direct solution
### Re-write SSD using vector matrix notation
$$J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$$
$$=\frac{1}{2m}(X\theta-y)^T(X\theta-y)$$
### where
$$X=\begin{bmatrix}
—(x^{(1)})^T— \\
—(x^{(2)})^T—\\
\vdots \\
—(x^{(n)})^T—
\end{bmatrix}\;\;
\vec{y}=\begin{bmatrix}
—y^{(1)}— \\
—y^{(2)}—\\
\vdots \\
—(y^{(n)})^T—
\end{bmatrix}
$$

### Solution: Normal Equation
$$\theta = (X^TX)^{-1}X^Ty$$

# Derivation of Normal Equations
### SSE in matrix form
$$J(\theta)=\frac{1}{2m}(X\theta-y)^T(X\theta-y)=$$
$$=\frac{1}{2m}(\theta^T(X^TX)\theta-2(X^Ty)^T\theta+const)$$
### Take gradient with respect to $\theta$ (vector), set to 0
$$\frac{\delta J}{\delta \theta}\propto (X^TX)^{-1}X^Ty$$
$$\theta=(X^TX)^{-1}X^Ty$$

**Also known as the *least mean squares*, or *least squares* solution**
![Pasted image 20221022162452.png|600](Pasted%20image%2020221022162452.png%7C600.md)
# Trade-offs
$m$ training examples, $n$ features.

## Gradient Descent
• No need to choose $\alpha$
• Don’t need to iterate
• Works well even when $n$ is large

## Normal Equations
- No need to choose $a$
- Don't need to iterate
- Need to compute $(X^TX)^{-1}$
**Computational complexity?**
- $O(n^3)$, slow if $n$ is very large

# Maximum Likelihood Principle (ML)
## So far, we have treated outputs as noiseless
- Defined cost function as “distance to true output”
- An alternate view:
	- data $(x, y)$ are generated by unknown process
	- however, we only observe a noisy version
	- how can we model this uncertainty?
- Alternative cost function?

## How to model uncertainty in data?
![Pasted image 20221022164801.png|300](Pasted%20image%2020221022164801.png%7C300.md)
### Hypothesis
$h_\theta(x)=\theta^Tx$
$\theta$: Parameters
$D=(x^{(i)}, y^{(i)}):$ data

### New cost function
maximize probability of given model:
$p((x^{(i)}, y^{(i)})|\theta)$

## Recall: Cost Function
![Pasted image 20221022164907.png|400](Pasted%20image%2020221022164907.png%7C400.md)

## Alternative view: Maximum Likelihood
![Pasted image 20221022165006.png|400](Pasted%20image%2020221022165006.png%7C400.md)

## Maximum Likelihood: Coin Toss Example
![869C46E2-BDF2-4A6D-8715-FBFDA5AD16AB copy.jpg](869C46E2-BDF2-4A6D-8715-FBFDA5AD16AB%20copy.jpg)

## Maximum Likelihood: Normal Distribution Example
- Observe a dataset of points $D=\{x^i\}_{i=1:10}$
- Assume $x$ is generated by **Normal distribution**, $x\sim N(x|\mu, \sigma)$
- Find parameters $\theta_{ML} = [\mu, \sigma]$ that maximize $\prod^{10}_{i=1}N(x^i|\mu,\sigma)$
![Pasted image 20221022174806.png|600](Pasted%20image%2020221022174806.png%7C600.md)
## Maximum likelihood way of estimating model parameters $\theta$
- In general, assume data is generated by some distribution $$U\sim p(U|\theta)$$
- Observations (i.i.d.) $$D=\{u^{(1)}, u^{(2)},...,u^{(m)}\}$$
- Maximum likelihood estimate
![Pasted image 20221022170625.png|600](Pasted%20image%2020221022170625.png%7C600.md)

### i.i.d. Observations
- i.i.d. == **i**ndependently **i**dentically **d**istributed random variables
- If $u^i$ are i.i.d. random variables then
$$p(u^1,u^2,...,u^m)=p(u^1)p(u^2)...p(u^m)$$
- A reasonable assumption about many datasets, but not always

# Maximum Likelihood for Linear Regression
## Recall: linear regression
Observed output is the true model’s output plus noise $$t^i=h_*(x^i)+\epsilon^i$$
![Pasted image 20221022183830.png|600](Pasted%20image%2020221022183830.png%7C600.md)
$p(t|x,\theta,\beta)=N(t|h(x),\beta^{-1})$

### Probability of of one data point {$x,t$}
$p(\boldsymbol{t}|\boldsymbol{x}, \theta, \beta)= \prod^m_{i=1}N(t^{(i)}|h(x^{(i)}),\beta^{-1})$    **Likelihood Function**

### Max. likelihood solution
$\theta_{ML}=\underset{\theta}{argmax}\; p(\boldsymbol{t}|\boldsymbol{x}, \theta, \beta)$
$\beta_{ML}=\underset{\beta}{argmax}\; p(\boldsymbol{t}|\boldsymbol{x}, \theta, \beta)$

### Want to maximize
$p(\boldsymbol{t}|\boldsymbol{x}, \theta, \beta)= \prod^m_{i=1}N(t^{(i)}|h(x^{(i)}),\beta^{-1})$

### Easier to maximize log()
$$ln\;p(\boldsymbol{t}|\boldsymbol{x}, \theta, \beta)=-\frac{\beta}{2}\sum_{i=1}^m(h(x^{(i)})-t^{(i)})^2+\frac{m}{2}ln\beta-\frac{m}{2}ln(2\pi)$$
### Want to *maximize* w.r.t. $\theta$
$$ln\;p(\boldsymbol{t}|\boldsymbol{x}, \theta, \beta)=-\frac{\beta}{2}\sum_{i=1}^m(h(x^{(i)})-t^{(i)})^2+\frac{m}{2}ln\beta-\frac{m}{2}ln(2\pi)$$
### But this is same as *minimizing* sum-of-squares cost
$$\frac{1}{2m}\sum_{i=1}^m(h(x^{(i)})-t^{(i)})^2$$

### Which is the same as our SSE cost from before!!
$$J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h(x^{(i)})-t^{(i)})^2$$
# Probabilistic Motivation for SSE
- Under the Gaussian noise assumption, maximizing the probability of the data points is the same as minimizing a sum-of-squares cost function
- The same as least squares method
- ML can be used for other hypotheses!
	- Note that linear regression has a closed-form solution while others may not
![Pasted image 20221022194803.png|600](Pasted%20image%2020221022194803.png%7C600.md)

# Why is $\beta$ useful for?
- Recall: we assumed observations $t$ are Gaussian given $h(x)$
- $\beta$ allows us to write down distribution over $t$, given new $x$, called **predictive distribution**
![Pasted image 20221022195253.png|400](Pasted%20image%2020221022195253.png%7C400.md)

# Predictive Distribution
Given a new input point $x$, we can now compute a distribution over the output $t$:
![79E5AFF7-0F8C-41B3-A7AD-F26AB1714528 copy.jpg|400](79E5AFF7-0F8C-41B3-A7AD-F26AB1714528%20copy.jpg%7C400.md)

# Non-linear Features
### Example
![Pasted image 20221022195704.png|600](Pasted%20image%2020221022195704.png%7C600.md)
$t=5e^x+2+noise$
	$h=28x-9$
change this to:
$t=5z+2+noise$
	$h=5z+1.8$

We transformed the date to make the relationship more linear by removing the exponential.

Do do this we use:

## Non-linear Basis Functions
Main idea: if the data is not linear, we can use a nonlinear mapping, or *basis function*, to transform the features to new ones
$$\phi(x):x\in R^N \rightarrow z\in R^M$$
- $M$ is the dimensionality of the new features/input $z$ (or $\phi(x)$)
- Note that $M$ could be $= N, \;>N$ or $<N$

# Polynomial basis functions
![Pasted image 20221022200116.png|600](Pasted%20image%2020221022200116.png%7C600.md)
![Pasted image 20221022200129.png|600](Pasted%20image%2020221022200129.png%7C600.md)


# Classification
## What to do if data is nonlinear?
### Example
#### Transform the input/feature
$$\phi(x):x\in\mathbb{R}^2\to z=x_1 \cdot x_2$$
#### Transformed training data: linearly separable!
![Pasted image 20221023131531.png|600](Pasted%20image%2020221023131531.png%7C600.md)

### Another Example
#### How to transform the input/feature?
![Pasted image 20221023131646.png|300](Pasted%20image%2020221023131646.png%7C300.md)
$\phi(x):x\mathbb{R}^2\to z=\begin{bmatrix}x^2\\x_1\cdot x_2\\x_2^2\end{bmatrix}$

#### Transformed training data: linearly separable
$y\in \{0,1\}$
0: "Negative Class" (e.g., benign tumor)
1: "Positive Class" (e.g., malignant tumor)

![Pasted image 20221022200916.png|600](Pasted%20image%2020221022200916.png%7C600.md)

Why not use least squares regression?
$$argmin\frac{1}{2m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$$
- Indeed, this is possible!
	- Predict 1 if $h_\theta(x)>.5, 0$ otherwise
- 

 # Things to learn
- Partial Derivatives
- ML & MLE
- Gradients & Descent
- Convolution

# Overfitting
![Pasted image 20221022200159.png|600](Pasted%20image%2020221022200159.png%7C600.md)

A disaster in overfitting:
![Pasted image 20221022200245.png|600](Pasted%20image%2020221022200245.png%7C600.md)

# Detecting overfitting
**Plot model complexity versus objective function on test/train data**

As model becomes more complex, performance on training keeps improving while on test data it gets worse

![Pasted image 20221022200405.png|300](Pasted%20image%2020221022200405.png%7C300.md)

**Horizontal axis:** *measure of model complexity*. In this example, we use the maximum order of the polynomial basis functions.

**Vertical axis:** For regression, it would be SSE or mean SE (MSE). For classification, the vertical axis would be classification error rate or cross-entropy error function.

# Overcoming overfitting
- Basic ideas
	- Use more training data
		  ![Pasted image 20221022200608.png|400](Pasted%20image%2020221022200608.png%7C400.md)
	- 
